<img src='https://github.com/fabiomatricardi/NuEtract1.5-streamlit/raw/main/article_logo.png' width=900>

# NuEtract1.5-stramlit
simple use of LLM extracting structured data from plain text

### What is NuExtract
**NuExtract-1.5-smol by NuMind üî•**<br>
NuExtract-1.5-smol is a fine-tuning of Hugging Face's SmolLM2-1.7B, intended for structured information extraction. It uses the same training data as NuExtract-1.5 and supports multiple languages, while being less than half the size (1.7B vs 3.8B).

To use the model, provide an input text and a JSON template describing the information you need to extract.

> Note: This model is trained to prioritize pure extraction, so in most cases all text generated by the model is present as is in the original text.

Check out the blog post.

It is also provided a tiny (0.5B) version which is based on Qwen2.5-0.5B: NuExtract-tiny-v1.5

> ‚ö†Ô∏è We recommend using NuExtract with a temperature at or very close to 0. Some inference frameworks, such as Ollama, use a default of 0.7 which is not well suited to pure extraction tasks.


### Description
using pure llama.cpp binaries to run NuExtract-1.5 models GGUF as a server

Streamlit app connected with openAI client to API endpoints

### Requirements
```
pip install tiktoken openai streamlit==1.40.1
```
You need also to download llama.cpp binaries from the [official repo](https://github.com/ggerganov/llama.cpp):
- [llama-b4150-bin-win-vulkan-x64.zip](https://github.com/ggerganov/llama.cpp/releases/download/b4150/llama-b4150-bin-win-vulkan-x64.zip) for Vulkan support
- [llama-b4150-bin-win-avx2-x64.zip](https://github.com/ggerganov/llama.cpp/releases/download/b4150/llama-b4150-bin-win-avx2-x64.zip) for AVX2 support

Extract the content into a subfolder called `llama.cpp`

---

<img src='https://github.com/fabiomatricardi/NuEtract1.5-streamlit/raw/main/savetheModelsDir.png' width=900>

Download the models in the subfolder `llama.cpp\model`
- [NuExtract-1.5-smol-Q5_K_L.gguf](https://huggingface.co/bartowski/NuExtract-1.5-smol-GGUF/resolve/main/NuExtract-1.5-smol-Q5_K_L.gguf)
- [numind.NuExtract-tiny-v1.5.Q8_0.gguf](https://huggingface.co/DevQuasar/numind.NuExtract-tiny-v1.5-GGUF/resolve/main/numind.NuExtract-tiny-v1.5.Q8_0.gguf)


### Usage
In one terminal window, inside the `llama.cpp` directory run
#### for NuExtract-1.5-smol based on SmolLM2-1.7b instruct
```
.\llama-server.exe --port 8001 -m .\model\NuExtract-1.5-smol-Q5_K_L.gguf -ngl 25 -c 8196
```
#### for NuExtract-tiny-v1.5 based on Qwen2.5-0.5B
```
.\llama-server.exe --port 8001 -m .\model\NuExtract-1.5-smol-Q5_K_L.gguf -ngl 25 -c 8196
```
<img src='https://github.com/fabiomatricardi/NuEtract1.5-streamlit/raw/main/2windows.png' width=900>


In another terminal window from the main project directory with the venv activated run
```
# for NuExtract-1.5-smol
streamlit run .\stapp1.5-nuextractSMOL.py
# NuExtract-tiny-v1.5
streamlit run .\stapp1.5-nuextractTINY.py
```

This the final running app

<img src='https://github.com/fabiomatricardi/NuEtract1.5-streamlit/raw/main/running_app.png' width=900>





